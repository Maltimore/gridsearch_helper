# Gridsearch Helper
This repository contains some helper scripts to launch your parallelized gridsearch with a scheduling system like sun grid engine (in fact at the moment only sun grid engine is supported).
Additionally it also provides a python package to collect and visualize the results of the gridsearch.

## Usage - running gridsearch jobs
Place this repository anywhere on your system. This program assumes that you have an entry point into your project that is a module called main.py, containing the function main(params), where params is a parameter dictionary.

```python
# filename: main.py

def main(params):
	# ...
```

It also assumes that in the same directory as your main.py you have a file called parameters.yaml in which you have two sections, one being 'default', containing the default parameters, and the other 'gridsearch', in which parameters to be gridsearched are given with their values in YAML-lists. See the example YAML file in this repository.

Then, from the working directory in which you want your program to be run, call the entry.sh script, which is called like

```bash
/path/to/entry.sh /path/to/main.py start_idx end_idx run_name
```

Where ``/path/to/main.py`` is the (relative or absolute) path to the main.py file, ``start_idx`` is the beginning index of the task ranage, ``end_idx`` is the end index of the task range, and ``run_name`` is the name for the entire run. To call a gridsearch with indices 1, 2, 3, 4, 5, 6 and name mygridsearch, and assuming that the (relative) path to your main.py is myprojet/main.py, call

```
/path/to/entry.sh myproject/main.py 1 6 mygridsearch_name
```

The results will be put in ``./outfiles/mygridsearch_name``, which is a path relative to your current working directory (your current working directory is also the working directory of the program you start with ```entry.sh```). Specifically, each instance of ```main(params)``` will get a params dictionary as argument. In params, you will find the keys 'gridsearch', set to ```True```, and 'output_path' and 'gridsearch_results_path'. 'output_path' is where you should save all relevant data generated by your program. Into 'gridsearch_results_path' you should only save a file 'program_state.yaml' in which you save the final score of the current run.

Hint: if you add the folder SGE from this repository to your PATH, you can easily call the gridsearch from anywhere on your system. Say that you have gridsearch_helper in your home directory, then you would do (for instance in your .bashrc):

```bash
export PATH=$PATH:"$HOME"/gridsearch_helper/SGE
```

and from then on you can call your gridsearch simply with ``gridsearch_entry.sh myrepo/src/main.py 1 20 mygridsearch_name``.

## Usage - analyzing results
This repository also contains a package ``gridsearch_analysis``. ``gridsearch_analysis`` can be installed by simply running ``pip install .`` from the root of this repository.
``gridsearch_analysis`` assumes that the above instructions were followed, in other words that there is a folder ``outfiles/mygridsearch_name/results`` and in this folder there is one subfolder for each job that ran, and in each subfolder there are two files: ``parameters.yaml`` and ``program_state.yaml``. You can use the function ``collect_results``, which loops over all subfolders and collects all values in ``parameters.yaml`` and ``program_state.yaml`` into a pandas dataframe.

If some of your runs didn't finish yet but you can't wait and already wnat to perform the analysis, you also have the option of setting some default values (see code below):

```python
from gridsearch_analysis import collect_results

# default values are being used if the run didn't finish yet and skip_unfinished_runs is False
DEFAULT_VALUES = {}
DEFAULT_VALUES['first_success'] = 150
RESULT_KEYS = ('first_success',)
skip_unfinished_runs = False

df = collect_results.collect_results(
	results_path, DEFAULT_VALUES, RESULT_KEYS, skip_unfinished_runs)
```


Furthermore, ``gridsearch_analysis`` can also make plots of the collected results. It will plot the performance as measured by some variable ``target_column`` as a function of user-specified indpendent variables ``relevant_parameters``. The type of plot created depends on how many independent variables you specify. If you have:
- 0  independent variables: create a swarm/barplot with a single group that contains all runs
- 1  independent variable: create a swarm/barplot with one group for each value of the independent variable
- 2+ independent variables: create a parallel coordinates plot

There is also an option to split the analysis into separate parts via the values of another independent variable. In this case, one plot per value of this ``split_analysis_column`` is performed. This effectively reduces the amount of independent variables by 1.

```python
from gridsearch_analysis import plotting

# Specify which columns to plot as strings in a list. List can be empty.
RELEVANT_PARAMETERS = ['name']  # list of strings (list can be empty)
# what variable to use as performance measure
TARGET_COLUMN = 'first_success'  # string
# is the performance better when the target variable is lower or when it is higher?
LOWER_IS_BETTER = True  # bool
# split up the analysis into separate parts for unique values in this column
SPLIT_ANALYSIS_COLUMN = None  # string or None

plotting.plot(
	df, plot_path, RELEVANT_PARAMETERS, TARGET_COLUMN, LOWER_IS_BETTER, SPLIT_ANALYSIS_COLUMN, VAR_ORDER)
```
